---
title: "Assignment_202320860"
author: "202320860"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
    toc_depth: 4
---
```{r global_options, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
```

My public Github repository for the final assignment's code can be found [here](https://github.com/BBBecky0913/MY472_Assignment2.git)

# 1 Introduction
A brief introduction to the research question and your approach to answering it. You do not need to cite any literature or write a literature review.
对研究问题的简要介绍以及回答这个问题的方法。你不需要引用任何文献或写一篇文献评论。
## 1.1 Research Question

## 1.2 Research Area
研究区域和时间
介绍几种研究尺度(在伦敦内部的)

id和name
*首先必须得知道的是PFAs**
London_PFAs <- c("metropolitan", "city-of-london")

加载数据，并排绘图
```{r}

```


## 1.3 Research Methods
```{r include=FALSE}
if(!require("tidyverse")) install.packages("tidyverse") 
if(!require("magrittr")) install.packages("magrittr") 
if(!require("lubridate")) install.packages("lubridate") 
if(!require("dplyr")) install.packages("dplyr") 
if(!require("stringr")) install.packages("stringr") 
if(!require("quanteda")) install.packages("quanteda") 
if(!require("tidyverse")) install.packages("tidyverse") 
if(!require("sf")) install.packages("sf") 
if(!require("RSQLite")) install.packages("RSQLite") 
if(!require("RSelenium")) install.packages("RSelenium") 
if(!require("netstat")) install.packages("netstat") 
if(!require("quanteda.textplots")) install.packages("quanteda.textplots") 
if(!require("httr")) install.packages("httr") 
if(!require("jsonlite")) install.packages("jsonlite") 
if(!require("quanteda.textstats")) install.packages("quanteda.textstats") 
if(!require("ggplot2")) install.packages("ggplot2") 
if(!require("ggrepel")) install.packages("ggrepel") 
if(!require("cowplot")) install.packages("cowplot")
if(!require("ggrepel")) install.packages("ggrepel") 
if(!require("RColorBrewer")) install.packages("RColorBrewer") 
if(!require("viridis")) install.packages("viridis") 
if(!require("wesanderson")) install.packages("wesanderson") 
if(!require("ggmap")) install.packages("ggmap") 
if(!require("ggspatial")) install.packages("ggspatial") 
if(!require("biscale")) install.packages("biscale") 
```

全局使用包
```{r results='hide'} 
# data operations
library(magrittr)
library(tidyverse)
library(dplyr, warn.conflicts = FALSE)
options(dplyr.summarise.inform = FALSE)

# texual analysis
library(stringr)
library(glue)
library(quanteda)
library(quanteda.textplots)
library(quanteda.textstats)

# data format transformation
library(lubridate)

# database inquiry
library(DBI)
library(RSQLite)

# web scraping
library(httr)
library(rvest)
library(RSelenium)
library(jsonlite)

# spatial analysis
library(sf)

# data visualization
library(ggplot2)
library(cowplot)
library(RColorBrewer)
library(viridis)
library(ggrepel)
library(ggmap)
library(ggspatial)

# local packages
# library(biscale)
```


## 1.4 😂 Research Design
包括图解
1 那几种空间统计和链接关系
LSOA（伦敦很多）、Neighborhood（仅做priority)
2 怎么融合

# 2 Data Description
A discussion of the data sources you used how you accessed them how you processed the data, the structure of your final analysis datasets) and so on.
讨论您使用的数据源、如何访问它们、如何处理数据、最终分析数据集的结构，等等。

## 2.1 Data Source
源址，访问（直接下载，爬取，API）
## 2.2 Data Processing
代空间位置信息数据的空间化
表格数据的转换
文本数据的清理（正则表达式）
## 2.3 Data Structure
文本数据，表数据，空间数据
数据库存储

# 3 Analysis
A presentation of your analysis including figures/graphs/maps and a discussion of your findings. In general, we do not expect you to conduct or interpret any formal statistical tests though you may do this if you wish. memember that your discussion should translate your specific analysis and results back to the level of the research question.
介绍你的分析，包括数字/图表/地图，并讨论你的发现。一般来说，我们不期望你进行或解释任 何正式的统计测试，但如果你愿意，你可以这样做。记住，你们的讨论应该将你们的具体分析和结果翻译回研究问题的层次。

```{r}
path_to_folder <- getwd()
```

## 3.1 💪 Identity Prejudice

### 🌹3.1.1 Basic Work

#### 1) Stop-and-Search Data Collection 

##### ① Gathering S&S Data from API
需要解释force id = PFA  force name = force
```{r}
London_PFAs <- c("metropolitan", "city-of-london")

# get the forces list in London. We can find that each Police Force Area(force_id in api json) corresponds to one police force(force_name)
force_list <- httr::content(GET("https://data.police.uk/api/forces"), "parsed") %>% lapply(unlist)

London_force_PFA <- data.frame()
London_forces <- c()

for (force in force_list){
  if (force["id"] %in% London_PFAs){
    force <- as.data.frame(t(force))
    London_force_PFA <- rbind(London_force_PFA, force)
  }
}

# 将force_name改写成api规定的url内部参数形式
# London_forces <- London_force_PFA$name <- do.call(c, lapply(London_force_PFA$name, function(x) gsub(" ", "-", x)))

London_force_PFA
# London_forces
```

介绍tibble表格里的变量含义
```{r eval=FALSE}
London_SS_tibble <- tibble()
year_dates <- "2023-11"

for (PFA in London_PFAs){
  for (year_date in year_dates){
    # get neighborhoods' urls during research period
    SS_url <- sprintf("https://data.police.uk/api/stops-force?force=%s&date=%s", PFA, year_date)
    SS_json <- SS_url %>% GET() %>%  httr::content("parsed")     # parse the content returned from our GET request
    for (SS in SS_json){
      SS <- SS %>% unlist() %>% t() %>% as_tibble()
      SS$datetime  <-  ymd_hms(SS$datetime)
      SS$PFA_name <- PFA
      SS$force <- London_force_PFA[London_force_PFA$id == PFA, "name"]
      London_SS_tibble <- bind_rows(London_SS_tibble, SS)  # bind_rows can combine two dfs with cols not exactly same
    }
  }
}
```

##### ② S&S Data Preprocessing
对生成的原始tibble进行了去除空值（9303->6906)、重命名和列重新排序后得到最终表格
some adjustments for the tibble
```{r eval=FALSE, include=FALSE}
# delete those rows which have at least a NA value in these columns written below
London_SS_tibble <- London_SS_tibble %>% 
                    filter(!is.na(longitude),
                           !is.na(latitude),
                           !is.na(gender),
                           !is.na(age_range),
                           !is.na(self_defined_ethnicity),
                           !is.na(officer_defined_ethnicity),
                           !is.na(object_of_search),
                           !is.na(outcome))

# rename some cols
London_SS_tibble <- London_SS_tibble %>%
  rename(
    street_id = location.street.id,
    street_name = location.street.name,
    longitude = location.longitude,
    latitude = location.latitude,
    outcome_object_id = outcome_object.id,
    outcome_object_name = outcome_object.name,
  )

# select and adjust the col sequence of tibble
London_SS_tibble <- London_SS_tibble %>% select(datetime, gender, age_range,
                                                self_defined_ethnicity,
                                                officer_defined_ethnicity, 
                                                longitude, latitude, 
                                                object_of_search, outcome,
                                                outcome_linked_to_object_of_search,
                                                removal_of_more_than_outer_clothing,
                                                street_id, street_name, force, PFA_name,
                                                type, involved_person,                                                                       legislation)
London_SS_tibble
```

```{r echo=FALSE, paged.print=TRUE}
# saveRDS(London_SS_tibble, "RDS_data/London_SS_tibble.rds")
London_SS_tibble<- readRDS( "RDS_data/London_SS_tibble.rds")
London_SS_tibble
```

#### 2) Chart Plot Template

##### ① Bar Chart
```{r eval=FALSE}
bar_plot <- function(data, x="", y, fill, 
                     is_facet = FALSE, 
                     viridis, is_discrete = TRUE,
                     title, title_size, ylab,
                     geom_text_size,
                     legend_lab, legend_item = c(),legend_title_size,
                     axis_title_size,
                     axis_text_size){
  
  plot <- plot <- ggplot(data, aes(x = x, y = y, fill = fill)) +
          geom_bar(stat = "identity") +
          labs(fill = legend_lab) +  
          scale_fill_viridis(option = viridis, direction = -1, discrete = is_discrete, begin = 0.4, end = 1) +
          xlab("") +
          ylab(ylab) + 
          ggtitle(title)+
          geom_text_repel(aes(label=y), vjust=1.6,
                          color="grey2", size=geom_text_size, alpha = 0.8) +
          theme(
            plot.title = element_text(size = title_size, face ='bold', hjust =0.08, vjust = 1),
            panel.background = element_blank(),
            panel.border = element_rect(colour = "black", fill = NA, linewidth = 0.5),
            strip.background = element_blank(),
            strip.text = element_text(size = 15, face ='bold'),
            legend.title = element_text(size = legend_title_size, face ='bold'),
            legend.text = element_text(size = 13),
            legend.key.size = unit(1, "cm"),
            legend.spacing.y = unit(0.3, "cm"),
            axis.title.x = element_text(size = axis_title_size[1]), 
            axis.title.y = element_text(size = axis_title_size[2]),  
            axis.text.x = element_text(size = axis_text_size[1]),   
            axis.text.y = element_text(size = axis_text_size[2]))
  
  if (is_facet == TRUE){
    # facet_wrap
    plot <- plot + facet_wrap(~ Var2, labeller = as_labeller(legend_item))
  }
  
  return(plot)
}
```
##### ② Pie Chart

```{r eval=FALSE}
pie_plot <- function(data, viridis, legend_lab, legend_loc){
  
  # first calculate the new variables (e.g. proportion) for plotting
  data <- data %>% 
    mutate(prop = Freq / sum(Freq) *100,
           prop_text = paste0(round(prop, 2), "%"),
           ypos = cumsum(prop)- 0.5*prop)
  
  # then plot
  plot <- ggplot(data, aes(x = "", y = Freq, fill = Var1)) +
          coord_polar("y") +
          # scale_fill_manual(values = palette) +
          scale_fill_viridis(option = viridis, direction = -1, discrete = TRUE, begin = 0.4, end = 1) +
          labs(fill = legend_lab) +  
          geom_col(color = "white") +
          geom_label_repel(aes(label = prop_text),
                     color = "black",
                     size = 7,
                     label.size = 0.9,
                     label.r = unit(0.3, "lines"),
                     label.padding = unit(0.8, "lines"),  # Amount of padding around label
                     position = position_stack(vjust = 0.5),
                     show.legend = FALSE) +
          theme_void() +
          theme(plot.margin = margin(t = 1.5, r = -10, 
                                     b = -0.8, l = -13.5, "cm"),
                legend.title = element_text(size = 20, face ='bold'),
                legend.text = element_text(size = 18),
                legend.key.size = unit(1, "cm"),
                legend.spacing.y = unit(0.3, "cm"), 
                legend.position = legend_loc)
  return(plot)
}

```
### 🌹3.1.2 Descriptive Statistics

#### 1) 🤢 Identity Structure
Identity: 
gender, age_range, (detailed) self_defined_ethnicity, (simplified) officer_defined_ethnicity

Crime type:
object_of_search（9）, legislation（4）

Crime severity (including mistake): 
outcome（7）, $removal_of_more_than_outer_clothing（2）

Misjudgment: (when the outcome is not linked to object or no crime)
outcome =  A no further action disposal,
outcome_linked_to_object_of_search

##### ① Extract the misjudged S&S
误判: 去除S&S判断正确的数据，排除某类特征人群的高违法率，能够更加真实的看出police对某一类人群的偏见  说明这个人不是真的违法只是遭到了嫌疑
"misjudged Stop and Search" 这个短语可能有几种含义，取决于上下文。在一般情况下，它可能指的是以下几个方面：

错误的判断：这可能意味着警察在执行 "Stop and Search"（即停止并搜查）操作时，基于错误或不充分的信息或依据做出了决定。例如，可能是基于错误的嫌疑或误解的情报而对某人进行了搜查。

种族或社会偏见：这个短语也可能指 "Stop and Search" 操作中体现的种族或社会偏见。例如，如果某个特定种族或社会群体在没有充分理由的情况下被过度搜查，这可能被认为是一种误判。

不恰当或非法的搜查：在某些情况下，"misjudged Stop and Search" 可能意味着搜查本身是不恰当的或违反了法律规定。这可能包括未经授权的搜查、超出法律许可范围的搜查，或者是在没有合理怀疑的情况下进行的搜查。

```{r paged.print=TRUE}
# 选取想要分类统计的列
SS_col_names <- c("gender", "age_range", "self_defined_ethnicity", "officer_defined_ethnicity", "object_of_search", "outcome", "outcome_linked_to_object_of_search", "removal_of_more_than_outer_clothing", "legislation")

SS_misjudgment <- London_SS_tibble %>% filter(outcome == "A no further action disposal")

# 迭代每一列，计算唯一值及其出现次数,将每个变量list转化为dataframe，以便制图
SS_col_counts <- SS_misjudgment[SS_col_names] %>% lapply(table) %>% lapply(data.frame)
SS_col_counts
```
##### ② Indentity Structure Statistics
- Gender, Age_range and Ethnicity (defined by self and officer)
```{r eval=FALSE, fig.width=22, fig.height=6}
gender_plot <- pie_plot(data = SS_col_counts$gender, 
                        legend_lab = "Gender",
                        viridis = "mako", 
                        legend_loc = c(1.02, 0.25))

age_range_plot <- pie_plot(data = SS_col_counts$age_range, 
                           viridis = "mako", 
                           legend_lab = "Age Range",
                           legend_loc = c(1.06, 0.28))

ethnicity_plot <- pie_plot(data = SS_col_counts$officer_defined_ethnicity,
                           viridis = "mako", 
                           legend_lab = "Ethnicity",
                           legend_loc = c(1.02, 0.25))

identity_plot <- plot_grid(gender_plot, age_range_plot, ethnicity_plot, nrow = 1)

final_identity_plot <- ggdraw() + 
              draw_plot(identity_plot) +
              draw_label('Pie Charts for Proportions of People with Different Identities under Stop-and-search', 
                         fontface = 'bold', size = 32, x = 0.5, 
                         vjust = -7.5, hjust = 0.5)
final_identity_plot
```

```{r echo=FALSE, fig.width=22, fig.height=6}
# saveRDS(final_identity_plot, "RDS_data/final_identity_plot.rds")
final_identity_plot<- readRDS( "RDS_data/final_identity_plot.rds")
final_identity_plot
```

- Data preprocessing
```{r}
ethnicity_detail <- SS_col_counts$self_defined_ethnicity

# 将self_defined_ethnicity中的大类和细分类分开成两列，便于后续的facet制图
ethnicity_detail$Var2 <- unlist(lapply(ethnicity_detail$Var1, function(x) gsub("(^[^-]*)-[^-]*", "\\1", x))) %>% trimws()  # 去除首末尾的空格

# Subclass
ethnicity_detail$Var1 <- unlist(lapply(ethnicity_detail$Var1, function(x) gsub("^[^-]*-", "", x))) %>% trimws()
```

- Plot Stacked Bar Chart for Proportions of Self-defined Ethnicity under Stop-and-search
```{r echo=FALSE, fig.width=11, fig.height=7.5}
# first calculate the new variables (e.g. proportion) for plotting
ethnicity_detail <- ethnicity_detail %>% 
mutate(prop = round(Freq / sum(Freq) *100, 1)) %>% 
# update factor levels of subclass according to the proportion in a descending order for following visualization (default is sorting by first letter)
arrange(desc(prop)) %>%
mutate(Var1 = factor(Var1, levels = Var1))
```

```{r eval=FALSE, fig.width=11, fig.height=7.5}
# 简化分类名
Ethnic_Class <- c("White" = "White related",
                  "Asian/Asian British" = "Asian related",
                  "Black/African/Caribbean/Black British" = "Black related",
                  "Mixed/Multiple ethnic groups" = "Mixed/ Multiple",
                  "Other ethnic group" = "Other group")

# choose stacked bar 
ethnicity_detail_plot <- bar_plot(data = ethnicity_detail,
                                  y = ethnicity_detail$prop, fill = ethnicity_detail$Var1,
                                  is_facet = TRUE,
                                  viridis = "mako",
                                  title = "Stacked Bar Chart for Proportions of Self-defined Ethnicity under Stop-and-search", 
                                  ylab = "Proportion (%)",
                                  title_size=19,
                                  geom_text_size = 5,
                                  legend_title_size = 14,
                                  axis_title_size = c(16,16),
                                  axis_text_size = c(12,12),
                                  legend_lab = "Self-defined Ethnicity",
                                  legend_item = Ethnic_Class)

ethnicity_detail_plot
```

```{r echo=FALSE, fig.width=11, fig.height=7.5}
# saveRDS(ethnicity_detail_plot, "RDS_data/ethnicity_detail_plot.rds")
ethnicity_detail_plot<- readRDS( "RDS_data/ethnicity_detail_plot.rds")
ethnicity_detail_plot
```

#### 2) 🤢 Identity Bias

##### ① Import 2021 Uk Census Data 
```{r}
# current path
path_to_folder_losa <- paste0(getwd(), "/All_data/0_LSOA_all_data/")

Age_range_census <- read_csv(paste0(path_to_folder_losa,"Age_5Bands.csv"))
Ethnic_group_census <- read.csv(paste0(path_to_folder_losa,'Ethnic_Group.csv'))
Sex_census <- read.csv(paste0(path_to_folder_losa,'Sex.csv'))
```

##### ② Zoom into London LSOAs
从ONS网站上获取了LSOA_2021_EW_BGC，并与伦敦边界裁剪得到London的LSOA空间数据
```{r}
# download the LSOA boundary data collected before
London_lsoa_sf <- read_sf(paste0(path_to_folder_losa,"London_LSOA.shp"))

Age_range_London_lsoa <- left_join(London_lsoa_sf, Age_range_census, by = c("LSOA21CD"= "geography code"))

Ethnic_group_London_lsoa <- left_join(London_lsoa_sf, Ethnic_group_census, by = c("LSOA21CD"= "geography.code"))

Sex_London_lsoa <- left_join(London_lsoa_sf, Sex_census, by = c("LSOA21CD"= "geography.code"))
```

##### ③ Summarise by S&S classification
```{r}
# summarise by age bands
Age_range_London_lsoa$Age_range_under_10  <- as_tibble(Age_range_London_lsoa) %>% 
  select("Age: Aged 4 years and under", "Age: Aged 5 to 9 years") %>% rowSums()

Age_range_London_lsoa$Age_range_10_19 <- as_tibble(Age_range_London_lsoa) %>% 
  select("Age: Aged 10 to 14 years", "Age: Aged 15 to 19 years") %>% rowSums()

Age_range_London_lsoa$Age_range_20_24 <- as_tibble(Age_range_London_lsoa) %>% 
  select("Age: Aged 20 to 24 years") %>% rowSums()

Age_range_London_lsoa$Age_range_25_34 <- as_tibble(Age_range_London_lsoa) %>% 
  select("Age: Aged 25 to 29 years", "Age: Aged 30 to 34 years") %>% rowSums()

Age_range_London_lsoa$Age_range_over_34 <- as_tibble(Age_range_London_lsoa) %>% 
  select("Age: Aged 35 to 39 years", "Age: Aged 40 to 44 years",
         "Age: Aged 45 to 49 years", "Age: Aged 50 to 54 years",
         "Age: Aged 55 to 59 years", "Age: Aged 60 to 64 years",
         "Age: Aged 65 to 69 years", "Age: Aged 70 to 74 years",
         "Age: Aged 75 to 79 years", "Age: Aged 80 to 84 years",
         "Age: Aged 85 years and over") %>% rowSums()
# calculate the overall mean proportions of age range
Age_range_prop_all <- Age_range_London_lsoa %>% summarise(pop_all = sum(`Age: Total`),
                                                `under 10` = mean(Age_range_under_10/ `Age: Total`),
                                                `10-17` = mean(Age_range_10_19/ `Age: Total`),
                                                `18-24` = mean(Age_range_20_24/ `Age: Total`),
                                                `25-34` = mean(Age_range_25_34/ `Age: Total`),
                                                `over 34` = mean(Age_range_over_34/ `Age: Total`)) %>% 
                                    as_tibble() %>% select(-geometry) %>% 
                                    pivot_longer(cols = c("under 10","10-17","18-24","25-34","over 34"), 
                                                 names_to = "Var1", 
                                                 values_to = "prop")

# calculate the proportions of Ethnic_group
Ethnic_London_lsoa <- Ethnic_group_London_lsoa %>% mutate(
                     Asian_prop = Ethnic.group..Asian..Asian.British.or.Asian.Welsh / Ethnic.group..Total..All.usual.residents,
                     Black_prop = Ethnic.group..Black..Black.British..Black.Welsh..Caribbean.or.African / Ethnic.group..Total..All.usual.residents,
                     White_prop = Ethnic.group..White / Ethnic.group..Total..All.usual.residents,
                     Mixed_prop = Ethnic.group..Mixed.or.Multiple.ethnic.groups / Ethnic.group..Total..All.usual.residents,
                     Other_prop = Ethnic.group..Other.ethnic.group / Ethnic.group..Total..All.usual.residents)
# summarise the overall mean proportion of Ethnic_group
Ethnic_prop_all <- Ethnic_group_London_lsoa %>% summarise(pop_all = sum(Ethnic.group..Total..All.usual.residents),
                                                Asian = mean(Asian_prop),
                                                Black = mean(Black_prop),
                                                White = mean(White_prop),
                                                Mixed = mean(Mixed_prop),
                                                Other = mean(Other_prop)) %>% 
                                    as_tibble() %>% select(-geometry) %>% 
                                    pivot_longer(cols = c("Asian","Black","White","Mixed", "Other"), 
                                                 names_to = "Var1", 
                                                 values_to = "prop")

# calculate the proportions of sex
Sex_London_lsoa <- Sex_London_lsoa %>% mutate(Male_prop = Sex..Male..measures..Value/ Sex..All.persons..measures..Value,
                                              Female_prop = Sex..Female..measures..Value/ Sex..All.persons..measures..Value)
# summarise the overall mean proportion of sex
Sex_prop_all <- Sex_London_lsoa %>% summarise(pop_all = sum(Sex..All.persons..measures..Value),
                                                Male = mean(Male_prop),
                                                Female = mean(Female_prop)) %>% 
                                    as_tibble() %>% select(-geometry) %>% 
                                    pivot_longer(cols = c("Male","Female"), 
                                                 names_to = "Var1", 
                                                 values_to = "prop")
```

```{r echo=FALSE}
# saveRDS(Age_range_London_lsoa, "RDS_data/Age_range_London_lsoa.rds")
Age_range_London_lsoa <- readRDS( "RDS_data/Age_range_London_lsoa.rds")
# saveRDS(Age_range_prop_all, "RDS_data/Age_range_prop_all.rds")
Age_range_prop_all <- readRDS( "RDS_data/Age_range_prop_all.rds")

# saveRDS(Ethnic_London_lsoa, "RDS_data/Ethnic_group_London_lsoa.rds")
Ethnic_London_lsoa <- readRDS( "RDS_data/Ethnic_group_London_lsoa.rds")
# saveRDS(Ethnic_prop_all, "RDS_data/Ethnic_prop_all.rds")
Ethnic_prop_all <- readRDS( "RDS_data/Ethnic_prop_all.rds")

# saveRDS(Sex_London_lsoa, "RDS_data/Sex_London_lsoa.rds")
Sex_London_lsoa <- readRDS( "RDS_data/Sex_London_lsoa.rds")
# saveRDS(Sex_prop_all, "RDS_data/Sex_prop_all.rds")
Sex_prop_all <- readRDS( "RDS_data/Sex_prop_all.rds")
```

##### ④ Disproportionality in S&S

- 计算身份特征在 "Stop and Search" 中的比例与在总人口中的比例之间的差异
```{r paged.print=TRUE}
Sex_prop_SS <- SS_col_counts$gender %>% 
    mutate(prop = Freq / sum(Freq), Var1 = as.character(Var1)) 

Age_range_prop_SS <- SS_col_counts$age_range %>% 
    mutate(prop = Freq / sum(Freq), Var1 = as.character(Var1))
Age_range_prop_SS

Ethnic_prop_SS <- ethnicity_detail %>% group_by(Var2) %>% summarise(Freq = sum(Freq)) %>% 
                   mutate(prop = Freq / sum(Freq),
                          Var1 = c("Asian", "Black", "Mixed", "Other", "White"))

Sex_disprop <- Sex_prop_SS %>% inner_join(Sex_prop_all, by = c("Var1" = "Var1")) %>% 
                               mutate(Sex_disprop = round(prop.x/prop.y - 1,2)) %>% select(Var1, Sex_disprop) %>% 
                               arrange(desc(Sex_disprop))
Sex_disprop

Age_range_disprop <- Age_range_prop_SS %>% inner_join(Age_range_prop_all, by = c("Var1" = "Var1")) %>% 
                               mutate(Age_range_disprop = round(prop.x/prop.y - 1,2)) %>% select(Var1, Age_range_disprop) %>% 
                               arrange(desc(Age_range_prop_SS))
Age_range_disprop

Ethnic_disprop <- Ethnic_prop_SS %>% inner_join(Ethnic_prop_all, by = c("Var1" = "Var1")) %>% 
                               mutate(Ethnic_disprop = round(prop.x/prop.y - 1,2)) %>% select(Var1, Ethnic_disprop) %>% 
                               arrange(desc(Ethnic_disprop))
Ethnic_disprop
```

```{r eval=FALSE, fig.width=20, fig.height=8}

Sex_disprop_bar <- bar_plot(data = Sex_disprop, x = Sex_disprop$Var1, 
                            y = Sex_disprop$Sex_disprop, fill = Sex_disprop$Sex_disprop,
                            viridis = "mako", is_discrete = FALSE,
                            title = " ", title_size = 50, ylab = "Disproportionality",
                            legend_lab = "Sex",
                            legend_title_size = 25,
                            geom_text_size = 6,
                            axis_title_size = c(18,18),
                            axis_text_size = c(22,22))

Age_range_disprop_bar <- bar_plot(data = Age_range_disprop, x = Age_range_disprop$Var1, 
                            y = Age_range_disprop$Age_range_disprop, fill = Age_range_disprop$Age_range_disprop,
                            viridis = "mako", is_discrete = FALSE,
                            title = " ", title_size = 50, ylab = "Disproportionality",
                            legend_lab = "Age",
                            legend_title_size = 25,
                            geom_text_size = 6,
                            axis_title_size = c(18,18),
                            axis_text_size = c(20,20))

Ethnic_disprop_bar <- bar_plot(data = Ethnic_disprop, x = Ethnic_disprop$Var1, 
                            y = Ethnic_disprop$Ethnic_disprop, fill = Ethnic_disprop$Ethnic_disprop,
                            viridis = "mako", is_discrete = FALSE,
                            title = " ", title_size = 50, ylab = "Disproportionality",
                            legend_lab = "Ethnicity",
                            legend_title_size = 25,
                            geom_text_size = 6,
                            axis_title_size = c(18,18),
                            axis_text_size = c(22,22))

disprop_plot <- plot_grid(Sex_disprop_bar, Age_range_disprop_bar, Ethnic_disprop_bar, nrow = 1)

final_disprop_plot <- ggdraw() + 
              draw_plot(disprop_plot) +
              draw_label('Different Identity Features Disproportionality in Stop-and-Search', 
                         fontface = 'bold', size = 28, x = 0.5, 
                         vjust = -12, hjust = 0.5)
final_disprop_plot
```

### 🌹3.1.3 🤢 Correlation Analysis
需要：九万个不能做，只能LSOA，所以要空间统计
因为x应该是地区的identity，如果越高说明有影响
三张普查表LSOA统计+
```{r}
London_lsoa_sf
```

```{r}
wards_levels <- wards %>% st_drop_geometry() %>% dplyr::select(starts_with("LEVEL"))

pairs(wards_levels)
correlations <- cor(wards_levels)
corrplot(correlations, method = "square", diag = FALSE, tl.col = "black")
```

## 3.2 💪 Spatial Inequality

### 🌹3.2.1 Basic Work

#### 1) Spatial Inequality Measurement


#### 2) Map Plot Template

##### ①  London Basemap

- Create bounding box according to the Greater London boundary
**把获取边界的London_neighborhoods_sf数据换成LSOA那个以免顺序错乱**
```{r eval=FALSE}
# get the key
APIKey <-  "fb69e364-41da-4456-89d6-e775f32a8076"
register_stadiamaps(key = APIKey, write = TRUE)

London_plot_bbox <- st_bbox(London_neighborhoods_sf)
names(London_plot_bbox) <- c("left", "bottom", "right", "top")

basemap_London <- get_stadiamap(London_plot_bbox, maptype = "stamen_toner_lite", 
                                source = "stadia", zoom = 12)
basemap_London <- ggmap(basemap_London) +
                       theme(line = element_blank(),
                             rect = element_blank(),
                             axis.title=element_blank(),
                             axis.text=element_blank())
                 
London_basemap_plot<- ggplot() +
                      annotation_custom(ggplotGrob(basemap_London), 
                                xmin=-Inf, xmax=Inf, ymin=-Inf, ymax=Inf) +
                      theme(rect = element_blank())

London_basemap_plot
```

```{r echo=FALSE}
# saveRDS(London_plot_bbox, "RDS_data/London_plot_bbox.rds")
London_plot_bbox <- readRDS( "RDS_data/London_plot_bbox.rds")

# saveRDS(London_basemap_plot, "RDS_data/London_basemap_plot.rds")
London_basemap_plot <- readRDS( "RDS_data/London_basemap_plot.rds")
```


##### ② Basic Map Style (不知道能不能合并)

### 🌹3.2.2 Spatial Data Visualization
#### 1) Stop-and-Search Heatmap
- tibble的空间数据转换
```{r}
# 将 convert London_SS_tibble to simple feature
London_SS_tibble$longitude <- as.numeric(London_SS_tibble$longitude)
London_SS_tibble$latitude <- as.numeric(London_SS_tibble$latitude)
London_SS_df <- as.data.frame(London_SS_tibble)
London_SS_sf <- st_as_sf(London_SS_df, coords = c("longitude", "latitude"), 
                         crs = st_crs(4326))

# add the coordinates into sf dataframe
coords <- st_coordinates(London_SS_sf)
London_SS_sf$longitude <- coords[, 'X']
London_SS_sf$latitude <- coords[, 'Y'] 
```

```{r eval=FALSE, fig.height=12, fig.width=15}

# 2D kernel density estimates were calculated using the stat_density2d() function in ggplot2, which automatically selects the bandwidth based on the standard deviation of the data and the number of data points.

SS_heatmap <- ggplot() +
  annotation_custom(ggplotGrob(London_basemap_plot), 
                    xmin=-Inf, xmax=Inf, ymin=-Inf, ymax=Inf) +
  labs(x ='', y ='', title = "Stop-and-search Records Heatmap in London") + 
  stat_density2d(data = London_SS_sf, aes(x=longitude, y=latitude,
                 fill=after_stat(level), alpha=after_stat(level)),
                 geom="polygon", adjust = 2) +  # bandwidth_size
  scale_alpha_continuous(range=c(0.25,0.65)) +
  scale_fill_gradientn(colours=rev(brewer.pal(7, "Spectral"))) +
  guides(fill=guide_legend("S&S Density"), alpha = "none") +
  geom_sf(data = London_SS_sf, color = 'black', size = 1.5, alpha = 0.3) +
  geom_sf(data = London_neighborhoods_sf, color = "black", 
          fill = NA, alpha = 0.8, size = 1) +
  coord_sf(xlim = London_plot_bbox[c(1,3)], ylim = London_plot_bbox[c(2,4)]) +
  theme(plot.title = element_text(size =35, hjust = 0.5, vjust = 1, face = "bold"),
        plot.margin = margin(t = 0.6, r = -12, 
                             b = 0, l = -12.5, "cm"),,
        legend.title = element_text(size = 25, face ='bold'),
        legend.position = c(0.9, 0.2),
        legend.key.size = unit(1.2, "cm"),
        legend.text = element_text(size = 22),
        legend.margin = margin(0, 2, 0, 2, "cm"),
        line = element_blank(), 
        rect = element_blank(), 
        axis.text=element_blank(),
        axis.title=element_blank(),
        axis.title.x = element_blank(),
        axis.title.y = element_blank()) +
        annotation_scale(location = "bl", line_width = 0.28,
                         pad_x = unit(0.025, "npc"), pad_y = unit(0.03, "npc"), 
                         height = unit(0.35, "cm"), text_cex = 2) + 
        annotation_north_arrow(location = "bl", which_north = "true", 
                               pad_x = unit(0.08, "npc"), pad_y = unit(0.08, "npc"),
                               height = unit(2.5, "cm"), width = unit(2, "cm")) 
```

```{r echo=FALSE, eval=FALSE, fig.height=12, fig.width=15}
# saveRDS(SS_heatmap, "RDS_data/SS_heatmap.rds")
SS_heatmap <- readRDS( "RDS_data/SS_heatmap.rds")
SS_heatmap
```

#### 2) Spatial Inequality Distribution

### 🌹3.2.3 Regression Analysis and Comparison

#### 1) Multiple Linear Regression


**OLS回归**
```{r}
# 使用lm函数进行OLS回归分析
model <- lm(y ~ x1 + x2, data = data)

# 查看回归结果摘要
summary(model)

# 进行模型诊断，比如画出残差图
plot(model)
```


#### 2) Geographically Weighted Regression

(变系数)
```{r}
library(spgwr)

# 假设你有一个空间点数据框df，包含坐标和其他变量
# df <- data.frame(lon = ..., lat = ..., var1 = ..., var2 = ..., ...)

# 将数据框转换为空间点对象
coordinates(df) <- ~ lon + lat

# 进行GWR，使用var1作为因变量，var2作为自变量
# 这里的bw是带宽，可以通过gwr.sel函数选择最优带宽
bw <- gwr.sel(var1 ~ var2, data = df, coords = cbind(df$lon, df$lat))
gwr_model <- gwr(var1 ~ var2, data = df, coords = cbind(df$lon, df$lat), bandwidth = bw)

# 查看GWR模型的结果
print(gwr_model)

# 如果需要，也可以进行模型的诊断和可视化

```

## 3.3 💪 Neighborhood Priority

### 🌹3.3.1 Basic Work
#### 1) Neighborhood Information Collection
##### ① Neighborhood List
根据PFA对应neighborhood
```{r}
# I get the lists of neighborhoods of different police force areas in Greater London, containing Metropolitan and City of London. The total number of neighborhoods in Greater London is 680.
London_neighborhoods_tibble <- tibble()

for (PFA in London_PFAs){
    # 获取不同county的neighborhood的url
    neighborhood_url <- sprintf("https://data.police.uk/api/%s/neighbourhoods", PFA)
    r <- GET(neighborhood_url)
    neighborhood_json <- httr::content(r, "parsed")     # parse the content returned from our GET request

    # 使用 lapply 函数, 关联neighbourhood对应的PFA
    neighborhood_json <- lapply(neighborhood_json, function(x) {
      x$PFA_name <- PFA
      return(x)
    })

    # 清除内层嵌套列表 但要注意这是一个列表是一行，如果和另一个县的元素rbind,是拥有293列的一行和拥有56列的一行相互合并，则后面一个县的元素被强行填充到293列（有很多县数据自动重复）,合并后586列
    neighborhood_list <- lapply(neighborhood_json, unlist)

    # 转换成tibble, 使用 lapply 转换每个元素为 data.frame，然后用 do.call rbind 到一起
    neighborhood_tibble <- neighborhood_list %>% 
    lapply(function(x) as.data.frame(t(as.data.frame(x)))) %>% 
    do.call(rbind, .) %>% 
    as_tibble()

    # 更改列名
    neighborhood_tibble <- neighborhood_tibble %>% 
       rename(neigh_id = id, neigh_name = name)

    # 合并不同county的neighborhood信息
    London_neighborhoods_tibble <- rbind(London_neighborhoods_tibble, neighborhood_tibble)
}

```

**为neighborhood关联相应的police force(根据前面简化)**
```{r}
# 转换成tibble, 使用 lapply 转换每个元素为 data.frame，然后用 do.call rbind 到一起
force_tibble <- force_list %>% 
lapply(function(x) as.data.frame(t(as.data.frame(x)))) %>% 
do.call(rbind, .) %>% 
as_tibble()

# 为tibble创建空白的force新列
London_neighborhoods_tibble$force <- NA

for (PFA in London_PFAs){
    # 根据PFA和police force name在london的对应关系进行赋值
    London_neighborhoods_tibble[London_neighborhoods_tibble$PFA_name == PFA, "force"] <- 
      force_tibble[force_tibble$id == PFA, "name"]
}
```

##### ① Neighborhood Boundaries
Crime type Priority怎么解释，怎么关联到object of search和text上

为了后续绘制地图，需要将neighborhood boundaries数据转换成sf数据需要的geometry格式，这边可以着重讲一下POLYGON类型的边界坐标数据是怎么存储的
耗时：4min, 23:55-00:00
```{r eval=FALSE}
# 为tibble创建空白的geometry新列
London_neighborhoods_tibble$geometry <- NA

for (neigh_id in London_neighborhoods_tibble$neigh_id){
    PFA <- London_neighborhoods_tibble[London_neighborhoods_tibble$neigh_id == neigh_id, "PFA_name"][[1]]
    neigh_boundary_url <- sprintf("https://data.police.uk/api/%s/%s/boundary", PFA, neigh_id)
    r <- GET(neigh_boundary_url)
    neigh_boundary_json <- httr::content(r, "parsed")     # parse the content returned from our GET request
    
    # 将neighborhood boundary 的坐标转化成POLYGON形式
      # 首先去除嵌套列表
    neigh_boundary_list <- neigh_boundary_json %>% lapply(unlist) %>% 
      # 其次去除向量的名字转化为纯numeric
    lapply(unname) %>% lapply(as.numeric) %>% 
      # 将列表转换为第一列为维度，第二列为经度的矩阵，最后整体转换成一个列表
    do.call(rbind, .) %>% list()

    # 对换boundary点集的经纬度坐标排列顺序
    neigh_boundary_list[[1]] <- neigh_boundary_list[[1]][, c(2, 1)]
      # 创建一个简单的多边形,并转换为用于存储几何信息的特殊列类型sfc, which适合嵌入到 tibble 或 data.frame中
    neigh_boundary_polygon <- neigh_boundary_list %>% st_polygon() %>% st_sfc()
    London_neighborhoods_tibble[London_neighborhoods_tibble$neigh_id == neigh_id, "geometry"][[1]] <- neigh_boundary_polygon
}

London_neighborhoods_tibble
```

```{r echo=FALSE, paged.print=TRUE}
# saveRDS(London_neighborhoods_tibble, "RDS_data/London_neighborhoods_tibble.rds")
London_neighborhoods_tibble<- readRDS( "RDS_data/London_neighborhoods_tibble.rds")
London_neighborhoods_tibble
```

转化为sf数据
```{r}
# 转换 tibble 为 sf 对象
London_neighborhoods_sf <- st_as_sf(London_neighborhoods_tibble, sf_column_name = "geometry", crs = 4326)
# 将 sf 对象写入 Shapefile
st_write(London_neighborhoods_sf, "All_data/London_neighborhoods/London_neighborhoods_sf.shp", append = FALSE)
```

#### 2) Map Plot template

现在这里制作一个plot template, 自定义色彩可视化数据(这个相关性再高必然有不好的分类，可以再一张图side-by-side：颜色+大小)
```{r}
library(biscale)
```

```{r eval=FALSE, fig.height=12, fig.width=15}
Bivariate_map_plot <- function(title, legend_loc, fill_layer){
  Bivariate_map <- 
  London_basemap_plot +
  labs(x ='', y ='', title = title) +
  geom_sf(data = London_neighborhoods_sf, inherit.aes = FALSE, aes(fill = fill_layer), color = 'black', size = 1.5, alpha = 1) +
  coord_sf(xlim = London_plot_bbox[c(1,3)], ylim = London_plot_bbox[c(2,4)]) +
  theme(plot.title = element_text(size =35, hjust = 0.5, vjust = 1, face = "bold"),
        plot.margin = margin(t = 0.6, r = -12, 
                             b = 0, l = -12.5, "cm"),
        legend.title = element_text(size = 25, face ='bold'),
        legend.position = legend_loc,
        legend.key.size = unit(1.2, "cm"),
        legend.text = element_text(size = 22),
        legend.margin = margin(0, 2, 0, 2, "cm"),
        line = element_blank(), 
        rect = element_blank(), 
        axis.text=element_blank(),
        axis.title=element_blank(),
        axis.title.x = element_blank(),
        axis.title.y = element_blank()) +
        annotation_scale(location = "bl", line_width = 0.2,
                         pad_x = unit(0.025, "npc"), pad_y = unit(0.03, "npc"), 
                         height = unit(0.32, "cm"), text_cex = 1.5) + 
        annotation_north_arrow(location = "bl", which_north = "true", 
                               pad_x = unit(0.08, "npc"), pad_y = unit(0.08, "npc"),
                               height = unit(2, "cm"), width = unit(1.5, "cm"))
  return(Bivariate_map)
}
```

### 🌹3.3.2 Neighborhood Priority
可以先考虑时间在202311年内的，如果大部分没有再考虑全部的priority

**注意：当赋值给tibble的列值是复杂数据结构时，用[[1]]才可以**
**后续文本分析把date, issue_text变成date, issue（几类）和 date, s&s 做时间线对比，参见之前的semianr code 有一个类似的！**

#### 1) Issue Texts Collection
获取priority_info（所有日期+文本）
- 通过什么什么api获取
```{r eval=FALSE}
# create a empty column to save spatial info for London_neighborhoods_tibble
London_neighborhoods_tibble$priority_info <- NA

for (neigh_id in London_neighborhoods_tibble$neigh_id){
    PFA <- London_neighborhoods_tibble[London_neighborhoods_tibble$neigh_id == neigh_id, "PFA_name"][[1]]
    neigh_priority_url <- sprintf("https://data.police.uk/api/%s/%s/priorities", PFA, neigh_id)
    r <- GET(neigh_priority_url)
    neigh_priority_json <- httr::content(r, "parsed") # parse the content returned from our GET request
    
    priority_info_list <- list()
    
    for (priority in neigh_priority_json){
      priority_date <- priority$`issue-date`  
      priority_info <- list(date = priority_date, issue_text = priority$issue)
      priority_info_list <- c(priority_info_list, list(priority_info))
    }
    if (length(priority_info_list) != 0){
      London_neighborhoods_tibble[London_neighborhoods_tibble$neigh_id == neigh_id, "priority_info"][[1]] <- list(priority_info_list)
    }
}

London_neighborhoods_tibble
```

```{r echo=FALSE, paged.print=TRUE}
# saveRDS(London_neighborhoods_tibble, "RDS_data/London_neighborhoods_tibble_1.rds")
London_neighborhoods_tibble<- readRDS("RDS_data/London_neighborhoods_tibble_1.rds")
London_neighborhoods_tibble
```

```{r}
library(quanteda)
library(quanteda.textplots)
library(quanteda.textstats)
```

#### 2) Texual Keywords Extraction

##### ①  Text Combination 

因为原数据是分年月日时间分开统计的，为了整体统计priority，首先合并Text
all year texts
写一个向量priority_doc，向量里是每个neighborhood的历年priority合并文本，名字是一个包含所有邻id里的向量
2min，合并同一neighborhood的文本
```{r eval=FALSE}
priority_doc <- c()
for (neigh_id in London_neighborhoods_tibble$neigh_id){
  issue_text_all <- c()
  
  neigh_priority <- London_neighborhoods_tibble[London_neighborhoods_tibble$neigh_id == neigh_id, "priority_info"][[1]]
  
  for (priority_date in neigh_priority[[1]]){
    issue_text <- priority_date$issue_text
    issue_text_all <- c(issue_text_all, issue_text)
  }
  neigh_combined_text <- paste(issue_text_all, collapse = " ")
  names(neigh_combined_text) <- neigh_id
  priority_doc <- c(priority_doc, neigh_combined_text)
}

head(priority_doc, n = 2L)
```

```{r echo=FALSE}
# saveRDS(priority_doc, "RDS_data/priority_doc.rds")
priority_doc <- readRDS("RDS_data/priority_doc.rds")
# writeLines(priority_doc, "priority_doc.txt")
head(priority_doc, n = 2L)
```

##### ② Tokenization and Cleaning
创建dfm统计分析关键词
标准化（Normalization）：这一步骤包括将所有tokens转换为标准格式，以减少数据的复杂性。常见的标准化技术包括小写化（转换为小写）、词干提取（stemming，减少单词到词干形式）、词形还原（lemmatization，将单词还原到其词典形式）。
分词（Tokenization）：这是将文本分割成单独tokens（如单词、短语或其他符号）的过程。例如，句子 "Natural language processing is interesting." 可以被分词为 "Natural", "language", "processing", "is", "interesting", 和 "."。

文本清洗：在分词之后，通常需要清洗文本，移除噪声数据，如标点符号、特殊字符、停用词（stop words，如"and", "the", "is" 等常见但通常不含信息量的单词）。

```{r paged.print=TRUE}
priority_docvars <- London_neighborhoods_tibble %>% dplyr::select(neigh_id, neigh_name, PFA_name, force) %>%
                    mutate(characters = str_count(priority_doc)) %>% 
                    as.data.frame()

priority_corpus <- corpus(priority_doc,
                       docvars = priority_docvars)

priority_corpus                      
docvars(priority_corpus)
```

清理文本特征，转化成dfm
```{r}
# transform corpus to document-feature matrix
text_mining <- function(corpus) {
  
    corpus_token <- corpus %>% 
    tokens(remove_punct = TRUE, remove_numbers = TRUE) %>%
    tokens_remove(stopwords("en")) %>%  # remove usual unnecessary words in English
# Whole Word Matching
      # Remove tokens that do not contain letters (pure numbers, etc.)
    tokens_remove(pattern = "\\b[^a-zA-Z]+\\b", valuetype = "regex") %>%  
      # Remove tokens containing numbers, regardless of position
    tokens_remove(pattern = "[0-9]+", valuetype = "regex") %>%  
      # Remove tokens containing non-word characters at the beginning and end, such as "-", "@"
    tokens_remove(pattern = "^\\W+|\\W+$", valuetype = "regex") %>% 
      # Remove tokens containing Chinese characters
    tokens_remove(pattern = "[\\u4E00-\\u9FFF]", valuetype = "regex") %>% 
      # Remove tokens consisting of 1 or 2 letters, which are generally not content words
    tokens_remove(pattern = "\\b[a-zA-Z]{1,2}\\b", valuetype = "regex") %>%  
    tokens_tolower()
    # steming the word to the basic form, like running and ran <- run
    corpus_token_stemed <- tokens_wordstem(corpus_token) 
    dfm <- corpus_token_stemed %>% dfm() %>% dfm_trim(min_termfreq = 5) # deletes all terms which are not in the corpus at least fifth, so that we can filter some address or people names
    result <- list(dfm, corpus_token, corpus_token_stemed)
    return(result)
}
   
```

```{r eval=FALSE}
# Get dfm, tokens and stemmed tokens for all neighborhoods of all years 
result_all <- text_mining(priority_corpus)
priority_dfm <- result_all[[1]]
priority_token <- result_all[[2]]
priority_token_stemed <- result_all[[3]]

priority_dfm
```

```{r echo=FALSE}
# saveRDS(priority_token, "RDS_data/priority_token.rds")
priority_token<- readRDS("RDS_data/priority_token.rds")

# saveRDS(priority_token_stemed, "RDS_data/priority_token_stemed.rds")
priority_token_stemed<- readRDS("RDS_data/priority_token_stemed.rds")

# saveRDS(priority_dfm, "RDS_data/priority_dfm.rds")
priority_dfm<- readRDS("RDS_data/priority_dfm.rds")
priority_dfm
```

```{r eval=FALSE, include=FALSE}
priority_dfm_df <- convert(priority_dfm, to = "data.frame")
# Transpose matrix
priority_dfm_mat <- t(priority_dfm_df)
col_names <- priority_dfm_T[1, ]
priority_dfm_df_T <- as.data.frame(priority_dfm_mat)
colnames(priority_dfm_df_T) <- col_names
priority_dfm_df_T <- priority_dfm_df_T[-1, ]
# Write data frame into csv
write.csv(priority_dfm_df_T, "priority_dfm_T.csv", row.names = TRUE)
```

创建一个查找表：对你的文本数据集进行词干提取。创建一个包含原始单词和它们的词干版本的映射。将这个映射保存为查找表。创建一个查找表，根据stemmed列的值合并original列的值，并以逗号分隔
tokens_wordstem和原单词对照表
This code creates a lookup table to make it easy to see which original words share the same stem.
```{r}
wordstem_lookup <- function(token, token_stemed){
  
  df_stemmed <- data.frame(original = unlist(token), 
                           stemmed = unlist(token_stemed))
  # keep only unique mappings
  df_unique <- df_stemmed[!duplicated(df_stemmed$original), ]
  
  # 创建一个查找表，根据stemmed列的值合并original列的值，并以逗号分隔：
  wordstem_lookup <- df_unique %>% 
    group_by(stemmed) %>%
    summarise(combined = toString(unique(original)))
  
  return(wordstem_lookup)
}

```

```{r eval=FALSE, paged.print=TRUE}
wordstem_lookup_list <- wordstem_lookup(priority_token, priority_token_stemed)
wordstem_lookup_list
```

```{r echo=FALSE, paged.print=TRUE}
# saveRDS(wordstem_lookup_list, "RDS_data/wordstem_lookup_list.rds")
wordstem_lookup_list<- readRDS("RDS_data/wordstem_lookup_list.rds")
wordstem_lookup_list
```

```{r eval=FALSE, include=FALSE}
# wordstem_lookup
write.csv(wordstem_lookup, "wordstem_lookup.csv", row.names = FALSE)
```

##### ③ Priority Keywords Definition
**dfm转换成dataframe,存储到本地进行观察,提取并统计主题词**
Convert dfm into dataframe, store it locally for observation, extract and count subject words
Crime Constitution
**通过观察object of search的类型，提取出三种主题词**
4张堆叠不分子图，上面两个名字type(先组合它们并取名），下面两个名字程度
Crime type:
object_of_search（9）, legislation（4）

Crime severity (including mistake): 
outcome（7）, $removal_of_more_than_outer_clothing（2）

需要自定义的项：图例参数（位置，标签大小)，标题大小
位置和间距）

- Plot Stacked Bar Chart for Proportions of Self-defined Ethnicity under Stop-and-search
```{r fig.width=18, fig.height=6}

object_of_search <- col_counts_df$object_of_search
legislation <- col_counts_df$legislation


outcome <- col_counts_df$outcome

# choose stacked bar
object_of_search_plot <- object_of_search %>% 
                         bar_plot(plot_type = "stacked",
                                  title = "", 
                                  legend_lab = "Object of Search")

# legislation_plot <- legislation %>% 
#                          bar_plot(plot_type = "stacked",
#                                   title = "", 
#                                   legend_lab = "Object of Search")

outcome_plot <- outcome %>% 
                         bar_plot(plot_type = "stacked",
                                  title = "", 
                                  legend_lab = "Object of Search")

identity_plot <- plot_grid(object_of_search_plot, outcome_plot, nrow = 1)
identity_plot

# final_identity_plot <- ggdraw() + 
#               draw_plot(identity_plot) +
#               draw_label('Pie Charts for Proportions of People with Different Identities under Stop-and-search', 
#                          fontface = 'bold', size = 32, x = 0.5, 
#                          vjust = -7.5, hjust = 0.5)
# final_identity_plot
```


```{r}
crime_keywords <- dictionary(list(ASB_keywords = c('violenc','damag','harass','distract', 'fight', 'combat', 'assault', 'graffiti', 'tension','speed','aggress', 'weapon', 'gang', 'knife', 'firework', 'children', 'women', 'girl', 'youth', 'femal', 'sexual', 'drinker', 'beggar'),
# P.S. speed: excess speed
                              
                              Drug_keywords = c('drug', 'deal', 'dealer', 'misus', 'abuse', 'cannabi', "nox", 'nitrous', 'oxid', 'substanc'),
# P.S. misus: misuse; NOx, oxid, nitrous: nitric oxide; canist: substanc: controlled substances(refer to drug)
                              
                              Theft_keywords = c('theft', 'steal', 'burglari', 'stolen', 'snatch', 'pickpocket', 'shoplift', 'fraud','tfmv', 'catalyt', 'bike', 'bicycl', 'keyless', 'parcel')))
# P.S. tfmv: theft from motor vehicles; Catalytic convertor theft
```

- Count the keywords in the neighborhood issue texts for all years and convert them into tibble
```{r}
priority_type <- priority_dfm %>% dfm_lookup(dictionary = crime_keywords) %>% 
                                     convert(to = "data.frame") %>% as_tibble()
```


### 🌹3.2.2 Potential Bias Analysis

#### 1) Priority and Different S&S Bivariate Map
需要着重介绍一下 Bivariate Map的概念尤其是后面绘图的算法
##### ①  对S&S data进行分类
- 按照列object_of_search的值对S&S data的S&S type进行分类，并存储在S_S_type列内。进一步将其按照S&S type分为三份数据便于后续分析
```{r eval=FALSE}
# The S&S type of S&S data is classified according to the value of column 'object_of_search' and stored in the column 'S_S_type'. It is further divided into three data according to S&S type to facilitate following analysis.

for (object in London_SS_sf$object_of_search) {
  if (object == "Controlled drugs") {
    London_SS_sf[London_SS_sf$object_of_search == object, "S_S_type"] <- "Drug"
  } else if (object == "Stolen goods") {
    London_SS_sf[London_SS_sf$object_of_search == object, "S_S_type"] <- "Theft"
  } else {
    London_SS_sf[London_SS_sf$object_of_search == object, "S_S_type"] <- "ASB"  # Anti-social Behaviour
  }
}

London_SS_ASB <- London_SS_sf %>% filter(S_S_type == "ASB") # 1805
                 
  
London_SS_Drug <- London_SS_sf %>% filter(S_S_type == "Drug") # 3794

London_SS_Theft <- London_SS_sf %>% filter(S_S_type == "Theft") # 1037
```

```{r echo=FALSE, paged.print=TRUE}
# saveRDS(London_SS_ASB, "RDS_data/London_SS_ASB.rds")
London_SS_ASB <- readRDS("RDS_data/London_SS_ASB.rds")

# saveRDS(London_SS_Drug, "RDS_data/London_SS_Drug.rds")
London_SS_Drug <- readRDS("RDS_data/London_SS_Drug.rds")

# saveRDS(London_SS_Theft, "RDS_data/London_SS_Theft.rds")
London_SS_Theft <- readRDS("RDS_data/London_SS_Theft.rds")
```

##### ② 对S&S data进行分类
空间关联到neighborhood

**既上次生成London_neighborhoods_sf存到shp，读取**
```{r eval=FALSE, include=FALSE}
# download the neighborhood boundary data generated before
London_neighborhoods_sf <- read_sf(paste0(path_to_folder,"/All_data/London_neighborhoods/London_neighborhoods_sf.shp"))
```

画bi_map需要把所有数据放到一个空间数据上，且每一个type需要是一列
```{r eval=FALSE}
  # left join the specific S&S type data (e.g. London_ASB_type) to the London_neighborhoods_sf with all information
neigh_spatial_join <- function(London_SS_type, type){
  neigh_SS_type <- London_neighborhoods_sf %>%
                st_join(London_SS_type, left = FALSE) %>%   
                group_by(neigh_id) %>%
                summarize(SS_count = n())
  
 # for loop the neigh_id of London_neighborhoods_sf, if the id of specific S&S type data is inside, create first time and update the specific column every time. Otherwise update the value with 0  
  for (neigh_ID in London_neighborhoods_sf$neigh_id){
    if (neigh_ID %in% neigh_SS_type$neigh_id){
      London_neighborhoods_sf[London_neighborhoods_sf$neigh_id == neigh_ID, 
                              sprintf("SS_%s_count", type)] <- 
        neigh_SS_type[neigh_SS_type$neigh_id == neigh_ID, "SS_count"][[1]]
    }
    else{London_neighborhoods_sf[London_neighborhoods_sf$neigh_id == neigh_ID, 
                                 sprintf("SS_%s_count", type)] <- 0}
  }
  return(London_neighborhoods_sf)
}

# 设置一个参数type，不同type对应该数据的不同字段名的更新
London_neighborhoods_sf <- neigh_spatial_join(London_SS_ASB, "ASB")
London_neighborhoods_sf <- neigh_spatial_join(London_SS_Drug, "Drug")
London_neighborhoods_sf <- neigh_spatial_join(London_SS_Theft, "Theft")
London_neighborhoods_sf
```

```{r echo=FALSE, paged.print=TRUE}
# saveRDS(London_neighborhoods_sf, "RDS_data/London_neighborhoods_sf.rds")
London_neighborhoods_sf <- readRDS("RDS_data/London_neighborhoods_sf.rds")
head(London_neighborhoods_sf, n =2L)
```

```{r eval=FALSE}
# first transform into dataframe to for table joining, then connect neighborhood priority and London_neighborhoods_sf with different types of s&s counts
London_neighborhoods_sf <- London_neighborhoods_sf %>% as.data.frame() %>% 
                           inner_join(priority_type, by = c("neigh_id" = "doc_id")) %>% 
# calculate the bi_class of three S&S types separately
                           bi_class(x = SS_ASB_count, 
                                    y = ASB_keywords, 
                                    style = "jenks", dim = 3) %>%   
                           rename("ASB_bi_class" = "bi_class") %>% 

                           bi_class(x = SS_Drug_count, 
                                    y = Drug_keywords, 
                                    style = "jenks", dim = 3) %>%
                           rename("Drug_bi_class" = "bi_class") %>% 


                           bi_class(x = SS_Theft_count, 
                                    y = Theft_keywords, 
                                    style = "jenks", dim = 3) %>%
                           rename("Theft_bi_class" = "bi_class") %>%
# transform back to sf
                           st_as_sf(sf_column_name = "geometry", crs = st_crs(4326))
  
```

```{r echo=FALSE}
# saveRDS(London_neighborhoods_sf, "RDS_data/London_neighborhoods_sf1.rds")
London_neighborhoods_sf <- readRDS("RDS_data/London_neighborhoods_sf1.rds")
London_neighborhoods_sf
```

Anti-social Behavior 
Higher frequency of neighborhood priority
Higher incidence of Stop-and-search
```{r eval=FALSE, fig.height=11, fig.width=13}
# bi_theme()
ASB_bivariate <- Bivariate_map_plot(title = "", legend_loc = "", fill_layer = London_neighborhoods_sf$ASB_bi_class) +
                 bi_scale_fill(pal = "GrPink", dim = 3) + labs(caption = "(Anti-social Behavior)") +
                 theme(plot.caption = element_text(hjust = 0.5, vjust = 1.8, size = 20))

Drug_bivariate <- Bivariate_map_plot(title = "", legend_loc = "", fill_layer = London_neighborhoods_sf$Drug_bi_class) +
                  bi_scale_fill(pal = "GrPink", dim = 3) + labs(caption = "(Controlled Drug)") +
                  theme(plot.caption = element_text(hjust = 0.5, vjust = 1.8, size = 20))

Theft_bivariate <- Bivariate_map_plot(title = "", legend_loc = "", fill_layer = London_neighborhoods_sf$Theft_bi_class) +
                   bi_scale_fill(pal = "GrPink", dim = 3) + labs(caption = "(Theft)") +
                   theme(plot.caption = element_text(hjust = 0.5, vjust = 1.8, size = 20))

# side by side
combined_plot_bivariate <- plot_grid(ASB_bivariate, Drug_bivariate, Theft_bivariate, ncol = 2)

final_plot_bivariate <- ggdraw() +
              draw_plot(combined_plot_bivariate) +
              draw_label('Bivariate Map of Stop-and-Search Types and Neighborhood Priority in London', fontface = 'bold', size = 25, x = 0.5, vjust = -19, hjust = 0.5)

legend <- bi_legend(pal = "GrPink",
                    dim = 3,
                    xlab = "Higher Neighborhood Priority",
                    ylab = "Higher Stop-And-Search",
                    size = 20)

# combine map with legend
final_plot_bivariate <- ggdraw() +
  draw_plot(final_plot_bivariate, 0, 0, 1, 1) +
  draw_plot(legend, 0.65, 0.12, 0.2, 0.2, scale = 2)  # left, bottom, right, top

final_plot_bivariate
```

```{r echo=FALSE, fig.height=11, fig.width=13}
# saveRDS(final_plot_bivariate, "RDS_data/final_plot_bivariate.rds")
final_plot_bivariate <- readRDS("RDS_data/final_plot_bivariate.rds")
final_plot_bivariate
```
#### 2) 😂 Priority in Research Period
时序共现图？
202311
直接写一个docvars，后面建corpus的时候只需再加一列字数，列包含；date, neigh_id, neigh_name, pfa, force, priority_text
```{r eval=FALSE}

# doc and docvars for corpus
priority_doc_202311 <- c()
priority_docvars_202311 <- data_frame()

for (neigh_ID in London_neighborhoods_tibble$neigh_ID){
   # get the detailed information of neighborhoods for docvars data
  neigh_name <- London_neighborhoods_tibble %>% dplyr::filter(neigh_ID == neigh_ID) %>% select(neigh_name) %>% pull()
  PFA_name <- London_neighborhoods_tibble %>% dplyr::filter(neigh_ID == neigh_ID) %>% select(PFA_name) %>% pull()
  force <- London_neighborhoods_tibble %>% dplyr::filter(neigh_ID == neigh_ID) %>% select(force) %>% pull()
  neigh_priority <- London_neighborhoods_tibble[London_neighborhoods_tibble$neigh_ID == neigh_ID, "priority_info"][[1]]
  
  for (priority_date in neigh_priority[[1]]){

    issue_date <- priority_date$date %>% ymd_hms()
    # Parse datetime string and extract year and month
    issue_ym <- format(issue_date, "%Y-%m")
    
    if (issue_ym == "2023-11"){
      
        issue_text <- priority_date$issue_text
        
        doc_name <- paste0(neigh_ID, "_", issue_date)
      
        if (doc_name %in% names(priority_doc_202311)){ 
        # if doc_name already exist then paste the new text to the former text and update the corresponding row in priority_doc_202311
          # combine text
          priority_doc_202311[doc_name] <- paste(priority_doc_202311[doc_name], issue_text)
          
          # update the text length in priority_docvars_202311
          priority_docvars_202311[priority_docvars_202311$neigh_ID == neigh_ID & priority_docvars_202311$priority_date == issue_date, "characters"] <- str_count(priority_doc_202311[doc_name])
        }
        
        else{
         # if doc_name doesn't exist then add a new row in priority_doc_202311
          priority_doc_202311[doc_name] <- issue_text

          neigh_date_priority_info <- data_frame("neigh_ID" = neigh_ID, 
                                                 "neigh_name" = neigh_name,
                                                 "priority_date" = issue_date,
                                                 "PFA_name" = PFA_name, "force" = force,
                                                 "characters" = str_count(priority_doc_202311[doc_name]))
          # add this row to priority_docvars_202311
          priority_docvars_202311 <- rbind(priority_docvars_202311, neigh_date_priority_info)
      }
    }
  }
}

head(priority_doc_202311, n = 2L)
```

```{r echo=FALSE}
# saveRDS(priority_doc_202311, "RDS_data/priority_doc_202311.rds")
priority_doc_202311<- readRDS("RDS_data/priority_doc_202311.rds")
head(priority_doc_202311, n = 2L)
# saveRDS(priority_docvars_202311, "RDS_data/priority_docvars_202311.rds")
priority_docvars_202311<- readRDS("RDS_data/priority_docvars_202311.rds")
```

```{r eval=FALSE}
# create corpus
priority_corpus_202311 <- corpus(priority_doc_202311,
                       docvars = priority_docvars_202311)

docvars(priority_corpus_202311)

# create and clean dfm
priority_dfm_202311 <- text_mining(priority_corpus_202311)[[1]]
priority_dfm_202311

# Check the number of neighborhoods that have S&S priority in Dec.2023: 103
length(unique(priority_dfm_202311$neigh_id))
```
```{r echo=FALSE}
# saveRDS(priority_dfm_202311, "RDS_data/priority_dfm_202311.rds")
priority_dfm_202311<- readRDS("RDS_data/priority_dfm_202311.rds")
priority_dfm_202311
```

 - Convert dfm to tibble for following analysis
```{r eval=FALSE}
# convert dfm into tibble
neigh_priority_key_202311 <- priority_dfm_202311 %>% 
                           dfm_lookup(dictionary = crime_keywords) %>% 
                           convert(to = "data.frame") %>% as_tibble()
neigh_priority_key_202311

# extract neigh_id and priority_date from doc_id
neigh_id <- unlist(lapply(neigh_priority_key_202311$doc_id, 
                          function(x) strsplit(x, "_")[[1]][1]))

priority_date <- unlist(lapply(neigh_priority_key_202311$doc_id, 
                          function(x) strsplit(x, "_")[[1]][2]))
# convert chr to standard date format
priority_date <- as.Date(unlist(lapply(priority_date, function(x) as.Date(x))))

neigh_priority_key_202311$neigh_id <- neigh_id
neigh_priority_key_202311$priority_date <- priority_date

```

```{r echo=FALSE, paged.print=TRUE}
# saveRDS(neigh_priority_key_202311, "RDS_data/neigh_priority_key_202311.rds")
neigh_priority_key_202311<- readRDS("RDS_data/neigh_priority_key_202311.rds")
neigh_priority_key_202311
```

3 做时序图与s&s共现：
text -> keyword -> s&s type
103个neighborhood分别是在哪天出现什么type，是否跟发布后当月或下个月的某类搜查的增长高度相关（有哪些neighborhood相关）
高度相关的衡量方法：issue出现的时间节点之后的某类搜查的数量比该月之前多（增长率很大）-->筛选排名前6（待定）的neighborhood做② 时序图
时序图构思：x轴为11月的时序（30天），y轴为s&s数目，按犯罪类型分子图分出3张子图，标出共现日期的点（点的大小是关键词出现数量
），对比不同neighborhood不同类型的犯罪问题对搜查增长的影响
需要一个表格：neighborhoodid，11月所有日（x轴30个点）（从一个有所有neighborhoodid的表格里提取出来，然后空白日期填0），s_s_count，asb_priority
现有表格：①：London_SS_ASB neighborhoodid，有s_s的日期
②  neigh_priority_key_202311：neigh_id， ASB_keywords，priority_date
新建一个表格：根据London_SS_ASB的neighborhoodid扩展，新建日期列30*id，for loop 如果表①的日期存在=1，否则=0；同理neigh_priority_key_202311

- This function is to add a column of corresponding neighborhood id for each S&S point.
```{r eval=FALSE}
Get_SS_neigh_id <- function(points_sf, polygons_sf){
  # use st_within() to find which neighborhood boundary each s&s point is located in
  matched <- st_within(points_sf, polygons_sf, sparse = FALSE)

  # create a new column to store polygon ID, apply a function that to each row of matched(1 means rows of a matrix)
  points_sf$neigh_id <- apply(matched, 1, function(x) {
    # Check if the current row (corresponding to one point) is inside any polygon
    if (any(x)) {
      # if yes, assign the current row value of polygons_sf$neigh_id to the corresponding value of points_sf$neigh_id
        # which(x) return the position indices of all TRUE values of bool vector x 
      return(polygons_sf$neigh_id[which(x)])
      } 
    else {return(NA)}
    }
  )
  return(points_sf)
}
```

```{r eval=FALSE}
London_SS_ASB <- Get_SS_neigh_id(London_SS_ASB, London_neighborhoods_sf)
London_SS_Drug <- Get_SS_neigh_id(London_SS_Drug, London_neighborhoods_sf)
London_SS_Theft <- Get_SS_neigh_id(London_SS_Theft, London_neighborhoods_sf)

# convert the datetime format into Y%-M%-D% format
London_SS_ASB$date <- as.Date(unlist(lapply(London_SS_ASB$datetime , function(x) {
  return(as.Date(paste0(year(x), "-", month(x), "-", day(x))))
})))

London_SS_Drug$date <- as.Date(unlist(lapply(London_SS_Drug$datetime , function(x) {
  return(as.Date(paste0(year(x), "-", month(x), "-", day(x))))
})))

London_SS_Theft$date <- as.Date(unlist(lapply(London_SS_Theft$datetime , function(x) {
  return(as.Date(paste0(year(x), "-", month(x), "-", day(x))))
})))
```

```{r echo=FALSE}
# saveRDS(London_SS_ASB, "RDS_data/London_SS_ASB.rds")
London_SS_ASB<- readRDS("RDS_data/London_SS_ASB.rds")
# London_SS_ASB

# saveRDS(London_SS_ASB, "RDS_data/London_SS_Drug.rds")
London_SS_Drug<- readRDS("RDS_data/London_SS_Drug.rds")
# London_SS_Drug

# saveRDS(London_SS_ASB, "RDS_data/London_SS_Theft.rds")
London_SS_Theft<- readRDS("RDS_data/London_SS_Theft.rds")
# London_SS_Theft

```

- count the specific S&S type and Priority grouped by neigh_id
```{r paged.print=TRUE}
# Keep the neighborhood IDs with non-zero occurrences of each theme word during the research period
neigh_priority_ASB <- neigh_priority_key_202311 %>% select(neigh_id, ASB_keywords) %>%
                      filter(!(ASB_keywords == 0)) %>% arrange(desc(ASB_keywords))
neigh_priority_ASB                         

neigh_priority_Drug <- neigh_priority_key_202311 %>% select(neigh_id, Drug_keywords) %>%
                       filter(!(Drug_keywords == 0)) %>% arrange(desc(Drug_keywords))
neigh_priority_Drug

neigh_priority_Theft <- neigh_priority_key_202311 %>% select(neigh_id, Theft_keywords) %>%
                        filter(!(Theft_keywords == 0)) %>% arrange(desc(Theft_keywords))
neigh_priority_Theft

# Count the different types of S&S for each neighborhood, remove neighborhood with NA values, and sort them in descending order

neigh_SS_ASB <- as_tibble(London_SS_ASB) %>% 
                    group_by(neigh_id) %>% 
                    summarise(ss_ASB_count = n()) %>% 
                    filter(!(is.na(neigh_id))) %>% 
                    arrange(desc(ss_ASB_count))
neigh_SS_ASB

neigh_SS_Drug <- as_tibble(London_SS_Drug) %>% 
                    group_by(neigh_id) %>% 
                    summarise(ss_Drug_count = n()) %>% 
                    filter(!(is.na(neigh_id))) %>% 
                    arrange(desc(ss_Drug_count))
neigh_SS_Drug

neigh_SS_Theft <- as_tibble(London_SS_Theft) %>% 
                    group_by(neigh_id) %>% 
                    summarise(ss_Theft_count = n()) %>% 
                    filter(!(is.na(neigh_id))) %>% 
                    arrange(desc(ss_Theft_count))
neigh_SS_Theft
```

- count the specific S&S type and Priority grouped by both neigh_id and date
```{r}
 # S&S
neigh_date_SS_ASB <- as_tibble(London_SS_ASB) %>% group_by(neigh_id, date) %>% summarise(ss_ASB_count = n())
neigh_date_SS_ASB

neigh_date_SS_Drug <- as_tibble(London_SS_Drug) %>% group_by(neigh_id, date) %>% summarise(ss_Drug_count = n())
neigh_date_SS_Drug

neigh_date_SS_Theft <- as_tibble(London_SS_Theft) %>% group_by(neigh_id, date) %>% summarise(ss_Theft_count = n())
neigh_date_SS_Theft

 # Priority
neigh_date_priority_ASB <- neigh_priority_key_202311 %>% group_by(neigh_id, priority_date) %>% summarise(keyword_ASB_count = sum(ASB_keywords))
neigh_date_priority_ASB

neigh_date_priority_Drug <- neigh_priority_key_202311 %>% group_by(neigh_id, priority_date) %>% summarise(keyword_Drug_count = sum(Drug_keywords))
neigh_date_priority_Drug

neigh_date_priority_Theft <- neigh_priority_key_202311 %>% group_by(neigh_id, priority_date) %>% summarise(keyword_Theft_count = sum(Theft_keywords))
neigh_date_priority_Theft
```

这么多个id太夸张了，用keyword的id（排名前几）来缩减！
① 筛选出London_SS_ASB和neigh_priority_ASB中同时存在的邻里id
② 将按neighbor_id 和 date计数的ss 和 priority左关联到一个expanded tibble上，未匹配的行赋值为0
- This function is to 
```{r}
Get_SS_time_series <- function(neigh_date_SS_type, neigh_date_priority_type){
  # ① find those neighborhoods which has specific type of S&S and priority at the same time
  neigh_id_ss_priority <- intersect(unique(neigh_date_SS_type$neigh_id), unique(neigh_date_priority_type$neigh_id))
  
  # ② create a date data sequence from 2023-11-01 to 2023-11-30
  dates_nov_2023 <- unlist(seq(from = as.Date("2023-11-01"), to = as.Date("2023-11-30"), by = "day"))
  
  # ③ create a tibble with each date in Dec as a row for each neighborhood 
  ss_priority_type <- as_tibble_col(neigh_id_ss_priority, column_name = "neigh_id") %>% crossing(date = dates_nov_2023)
  
  # ④ left_join() keeps all observations in ss_priority_type, if matched, get the S&S data, else remain NA
  ss_priority_type <- left_join(ss_priority_type, neigh_date_SS_type, by = c("neigh_id" = "neigh_id", "date" = "date")) 
  ss_priority_type <- left_join(ss_priority_type, neigh_date_priority_type, by = c("neigh_id" = "neigh_id", "date" = "priority_date"))
  
  # ⑤ convert all NA to 0
  ss_priority_type <- ss_priority_type %>%
                      mutate(across(everything(), ~ replace_na(., 0)))
  
  # ⑥ add neighborhood names to the tibble
  neigh_id_name <- London_neighborhoods_tibble %>% select(neigh_id, neigh_name)
  ss_priority_type <- left_join(ss_priority_type, neigh_id_name, by = c("neigh_id" = "neigh_id")) 
  
  return(ss_priority_type)
}
```

```{r}
ss_priority_ASB <- Get_SS_time_series(neigh_date_SS_ASB, neigh_date_priority_ASB)
ss_priority_ASB

ss_priority_Drug <- Get_SS_time_series(neigh_date_SS_Drug, neigh_date_priority_Drug)
ss_priority_Drug

ss_priority_Theft <- Get_SS_time_series(neigh_date_SS_Theft, neigh_date_priority_Theft)
ss_priority_Theft
```

```{r}
# saveRDS(ss_priority_ASB, "RDS_data/ss_priority_ASB.rds")
ss_priority_ASB<- readRDS("RDS_data/ss_priority_ASB.rds")
# ss_priority_ASB

# saveRDS(ss_priority_Drug, "RDS_data/ss_priority_Drug.rds")
ss_priority_Drug<- readRDS("RDS_data/ss_priority_Drug.rds")
# ss_priority_Drug

# saveRDS(ss_priority_Theft, "RDS_data/ss_priority_Theft.rds")
ss_priority_Theft<- readRDS("RDS_data/ss_priority_Theft.rds")
# ss_priority_Theft
```

```{r}
outlier <- ss_priority_ASB %>% filter(neigh_id == "E05013806N")

# 去掉异常值
ss_priority_ASB <- ss_priority_ASB %>% filter(!(neigh_id == "E05013806N"))
```

那得三个月的数据才能看出来吧？前后两个月
```{r}
# 假设df是你的数据框

# 添加分组变量
ss_priority_ASB$group <- cut(ss_priority_ASB$neigh_id, breaks = 3, labels = c("Group1", "Group2", "Group3"))

# 根据分组变量绘图
p <- ggplot(df, aes(x = x, y = y, group = sample_id)) + 
     geom_line() +
     facet_wrap(~ group)

# 打印或保存绘图
print(p)

```

```{r}
ss_priority_ASB_1 <- slice(ss_priority_ASB, 1:(20*30))
ss_priority_ASB_1$sample_id <- 1
ss_priority_ASB_2 <- slice(ss_priority_ASB, (21*30):(40*30))
ss_priority_ASB_2$sample_id <- 2
ss_priority_ASB_3 <- slice(ss_priority_ASB, (41*30):(60*30))
ss_priority_ASB_3$sample_id <- 3
ss_priority_ASB_4 <- slice(ss_priority_ASB, (61*30):(40*77))
ss_priority_ASB_4$sample_id <- 1

ss_priority_ASB <- rbind(ss_priority_ASB_1, ss_priority_ASB_2, ss_priority_ASB_3, ss_priority_ASB_4)
```
三个类型选几个得了（先分批，再选一个放出来看看）
```{r eval=FALSE, fig.height=5, fig.width=10}

Time_series_plot <- function(){
  SS_linechart_type <-  
  ggplot() +  
  geom_line(data = ss_priority_ASB_2, 
            aes(x = date,
                y = ss_ASB_count,
                color = neigh_name),
            linewidth = 0.8) +
  geom_point(data = ss_priority_ASB_2,
             aes(x = date,
                 y = ss_ASB_count,
                 color = neigh_name,
                 size = keyword_ASB_count)) +
  scale_color_viridis(option = "turbo", direction = -1, discrete = TRUE) +
  labs(title = "Relationship between S&S and Priority for %s in time series", 
       x = "Date", 
       y = "The number of S&S") + 
  # scale_color_manual(values = c('C6M_mean' = 'blue4', 'E1_mean' = 'pink3'),
  #                    labels = c('C6M_mean' = 'Stay-at-home Restrictions', 'E1_mean' = 'Income Support')) +
  theme(
      rect = element_blank(), # remove background
      plot.title = element_text(size =12, hjust =0.5, vjust = 0.5),
      legend.position = "right",
      strip.text = element_text(size = 10), 
      panel.spacing = unit(1, "lines"),
      panel.border = element_rect(colour = "black", fill=NA, linewidth=0.5),
      # legend.position = "",
      legend.background = element_blank(),
      legend.key = element_rect(fill = "transparent"),
      legend.key.size = unit(0.5, "cm"),
      legend.text = element_text(size = 10)
      ) 
  SS_linechart_type
return(SS_linechart_type)
}

```
# 4 Conclusion
总结到底是因为什么偏见
# 5 Reference

# 6 Code Appendix
```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE} 
# this chunk generates the complete code appendix. 
```
